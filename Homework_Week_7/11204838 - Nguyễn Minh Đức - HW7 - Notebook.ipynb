{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6934099,"sourceType":"datasetVersion","datasetId":3981600},{"sourceId":6982422,"sourceType":"datasetVersion","datasetId":4012823},{"sourceId":6982870,"sourceType":"datasetVersion","datasetId":4013121}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom PIL import Image\nimport cv2\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T16:11:54.222290Z","iopub.execute_input":"2023-11-16T16:11:54.222692Z","iopub.status.idle":"2023-11-16T16:11:54.228495Z","shell.execute_reply.started":"2023-11-16T16:11:54.222660Z","shell.execute_reply":"2023-11-16T16:11:54.227360Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\n! pip install --quiet ultralytics\n\nfrom ultralytics import YOLO\n\nobject_detection_model = YOLO(\"yolov8n.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T15:42:05.835952Z","iopub.execute_input":"2023-11-16T15:42:05.836678Z","iopub.status.idle":"2023-11-16T15:42:19.563200Z","shell.execute_reply.started":"2023-11-16T15:42:05.836644Z","shell.execute_reply":"2023-11-16T15:42:19.561998Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2023-11-16 15:42:06--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\nResolving github.com (github.com)... 192.30.255.113\nConnecting to github.com (github.com)|192.30.255.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/1013abe2-be6e-4606-8433-daf2baecf594?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231116T154206Z&X-Amz-Expires=300&X-Amz-Signature=864fe3101e978bebe8313af6c59ae70862b0dc7085ea9bb48aaca6298025f7db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8n.pt&response-content-type=application%2Foctet-stream [following]\n--2023-11-16 15:42:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/1013abe2-be6e-4606-8433-daf2baecf594?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231116T154206Z&X-Amz-Expires=300&X-Amz-Signature=864fe3101e978bebe8313af6c59ae70862b0dc7085ea9bb48aaca6298025f7db&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8n.pt&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6534387 (6.2M) [application/octet-stream]\nSaving to: ‘yolov8n.pt.1’\n\nyolov8n.pt.1        100%[===================>]   6.23M  --.-KB/s    in 0.08s   \n\n2023-11-16 15:42:07 (82.6 MB/s) - ‘yolov8n.pt.1’ saved [6534387/6534387]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm yolo_dataset_1.yaml # nếu có file thì xóa\n!echo 'train: /kaggle/input/dataset-for-yolo/data/images/train' >> yolo_dataset_1.yaml\n!echo 'val: /kaggle/input/dataset-for-yolo/data/images/val' >> yolo_dataset_1.yaml\n!echo 'nc: 10' >> mydataset_1.yaml\n!echo \"names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\" >> yolo_dataset_1.yaml","metadata":{"execution":{"iopub.status.busy":"2023-11-16T15:34:15.295959Z","iopub.execute_input":"2023-11-16T15:34:15.296260Z","iopub.status.idle":"2023-11-16T15:34:19.992125Z","shell.execute_reply.started":"2023-11-16T15:34:15.296235Z","shell.execute_reply":"2023-11-16T15:34:19.990955Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"rm: cannot remove 'yolo_dataset_1.yaml': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"result_object_detection = object_detection_model.train(data=\"/kaggle/working/yolo_dataset_1.yaml\", epochs=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T15:42:32.013417Z","iopub.execute_input":"2023-11-16T15:42:32.014474Z","iopub.status.idle":"2023-11-16T16:08:21.174321Z","shell.execute_reply.started":"2023-11-16T15:42:32.014438Z","shell.execute_reply":"2023-11-16T16:08:21.173122Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Ultralytics YOLOv8.0.210 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/working/yolo_dataset_1.yaml, epochs=2, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\nOverriding model.yaml nc=80 with nc=10\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \nModel summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-for-yolo/data/labels/train... 55000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55000/55000 [00:47<00:00, 1168.99it/s]\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /kaggle/input/dataset-for-yolo/data/images/train/11290.png: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /kaggle/input/dataset-for-yolo/data/images/train/15247.png: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /kaggle/input/dataset-for-yolo/data/images/train/31900.png: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /kaggle/input/dataset-for-yolo/data/images/train/54614.png: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /kaggle/input/dataset-for-yolo/data/images/train/54735.png: 1 duplicate labels removed\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/dataset-for-yolo/data/labels is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-for-yolo/data/labels/val... 5000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:04<00:00, 1168.80it/s]\n\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/dataset-for-yolo/data/labels is not writeable, cache not saved.\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 2 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        1/2      4.47G     0.9201      1.459      1.242         30        640: 100%|██████████| 3438/3438 [11:07<00:00,  5.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:32<00:00,  4.82it/s]\n                   all       5000      10000      0.987      0.971      0.993      0.899\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        2/2      4.49G     0.6334     0.7328      1.048         30        640: 100%|██████████| 3438/3438 [10:27<00:00,  5.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:31<00:00,  4.95it/s]\n                   all       5000      10000      0.993      0.984      0.995      0.949\n\n2 epochs completed in 0.379 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics YOLOv8.0.210 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\nModel summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:36<00:00,  4.31it/s]\n                   all       5000      10000      0.993      0.984      0.995      0.949\n                     0       5000        976      0.993      0.993      0.995      0.947\n                     1       5000       1000       0.99      0.978      0.994      0.955\n                     2       5000       1048      0.998      0.985      0.995      0.925\n                     3       5000        980      0.996      0.983      0.995      0.956\n                     4       5000       1012          1       0.98      0.995      0.957\n                     5       5000        978      0.998      0.984      0.995      0.955\n                     6       5000        982      0.992      0.981      0.995      0.953\n                     7       5000        963       0.98      0.981      0.994      0.949\n                     8       5000       1035      0.991      0.986      0.995      0.944\n                     9       5000       1026      0.995      0.984      0.995      0.953\nSpeed: 0.2ms preprocess, 1.9ms inference, 0.0ms loss, 0.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁█</td></tr><tr><td>lr/pg1</td><td>▁█</td></tr><tr><td>lr/pg2</td><td>▁█</td></tr><tr><td>metrics/mAP50(B)</td><td>▁█</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁█</td></tr><tr><td>metrics/precision(B)</td><td>▁█</td></tr><tr><td>metrics/recall(B)</td><td>▁█</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▁</td></tr><tr><td>train/cls_loss</td><td>█▁</td></tr><tr><td>train/dfl_loss</td><td>█▁</td></tr><tr><td>val/box_loss</td><td>█▁</td></tr><tr><td>val/cls_loss</td><td>█▁</td></tr><tr><td>val/dfl_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00024</td></tr><tr><td>lr/pg1</td><td>0.00024</td></tr><tr><td>lr/pg2</td><td>0.00024</td></tr><tr><td>metrics/mAP50(B)</td><td>0.99471</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.94943</td></tr><tr><td>metrics/precision(B)</td><td>0.99325</td></tr><tr><td>metrics/recall(B)</td><td>0.98362</td></tr><tr><td>model/GFLOPs</td><td>8.204</td></tr><tr><td>model/parameters</td><td>3012798</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.841</td></tr><tr><td>train/box_loss</td><td>0.63336</td></tr><tr><td>train/cls_loss</td><td>0.7328</td></tr><tr><td>train/dfl_loss</td><td>1.0481</td></tr><tr><td>val/box_loss</td><td>0.32399</td></tr><tr><td>val/cls_loss</td><td>0.21998</td></tr><tr><td>val/dfl_loss</td><td>0.83057</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">train</strong> at: <a href='https://wandb.ai/duc_nguyen_02/YOLOv8/runs/m481249q' target=\"_blank\">https://wandb.ai/duc_nguyen_02/YOLOv8/runs/m481249q</a><br/>Synced 6 W&B file(s), 21 media file(s), 5 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231116_153448-m481249q/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"predictions = object_detection_model.predict(\"/kaggle/input/dataset-for-yolo/data/images/val/100.png\", save = True)\nimg = Image.open(\"/kaggle/working/runs/detect/train6/100.png\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T16:12:46.353750Z","iopub.execute_input":"2023-11-16T16:12:46.354507Z","iopub.status.idle":"2023-11-16T16:12:46.563794Z","shell.execute_reply.started":"2023-11-16T16:12:46.354474Z","shell.execute_reply":"2023-11-16T16:12:46.562807Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\nimage 1/1 /kaggle/input/dataset-for-yolo/data/images/val/100.png: 640x640 1 0, 1 6, 7.6ms\nSpeed: 2.1ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\nResults saved to \u001b[1mruns/detect/train7\u001b[0m\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7e129f46f6a0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"object_segmentation_model = YOLO(\"yolov8n-seg.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T16:15:22.687053Z","iopub.execute_input":"2023-11-16T16:15:22.687435Z","iopub.status.idle":"2023-11-16T16:15:22.745553Z","shell.execute_reply.started":"2023-11-16T16:15:22.687406Z","shell.execute_reply":"2023-11-16T16:15:22.744531Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!rm yolo_dataset_2.yaml # nếu có file thì xóa\n!echo 'train: /kaggle/input/dataset-for-yolov8-seg/data_segmetation/images' >> yolo_dataset_2.yaml\n!echo 'val: /kaggle/input/dataset-for-yolov8-seg/data_segmetation/masks' >> yolo_dataset_2.yaml\n!echo 'nc: 10' >> mydataset_2.yaml\n!echo \"names: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\" >> yolo_dataset_2.yaml\n!echo \"segmentation_classes: 10\" >> yolo_dataset_2.yaml","metadata":{"execution":{"iopub.status.busy":"2023-11-16T16:26:39.214737Z","iopub.execute_input":"2023-11-16T16:26:39.215481Z","iopub.status.idle":"2023-11-16T16:26:45.549034Z","shell.execute_reply.started":"2023-11-16T16:26:39.215449Z","shell.execute_reply":"2023-11-16T16:26:45.547458Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"result_object_segmentation = object_segmentation_model.train(data=\"/kaggle/working/yolo_dataset_2.yaml\", epochs=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T16:26:48.553411Z","iopub.execute_input":"2023-11-16T16:26:48.554408Z","iopub.status.idle":"2023-11-16T16:28:23.807345Z","shell.execute_reply.started":"2023-11-16T16:26:48.554363Z","shell.execute_reply":"2023-11-16T16:28:23.805654Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Ultralytics YOLOv8.0.210 🚀 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/kaggle/working/yolo_dataset_2.yaml, epochs=2, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train5\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1   1006030  ultralytics.nn.modules.head.Segment          [10, 32, 64, [64, 128, 256]]  \nYOLOv8n-seg summary: 261 layers, 3265566 parameters, 3265550 gradients, 12.1 GFLOPs\n\nTransferred 417/417 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train5', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dataset-for-yolov8-seg/data_segmetation/labels... 0 images, 5000 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:01<00:00, 3407.52it/s]\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ No labels found in /kaggle/input/dataset-for-yolov8-seg/data_segmetation/labels.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/dataset-for-yolov8-seg/data_segmetation is not writeable, cache not saved.\nWARNING ⚠️ No labels found in /kaggle/input/dataset-for-yolov8-seg/data_segmetation/labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/dataset-for-yolov8-seg/data_segmetation/masks... 0 images, 5000 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:01<00:00, 3190.65it/s]\n\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ No labels found in /kaggle/input/dataset-for-yolov8-seg/data_segmetation/masks.cache. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/dataset-for-yolov8-seg/data_segmetation is not writeable, cache not saved.\nWARNING ⚠️ No labels found in /kaggle/input/dataset-for-yolov8-seg/data_segmetation/masks.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\nPlotting labels to runs/segment/train5/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n","output_type":"stream"},{"name":"stdout","text":"zero-size array to reduction operation maximum which has no identity\n","output_type":"stream"},{"name":"stderr","text":"Image sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/segment/train5\u001b[0m\nStarting training for 2 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        1/2       7.9G          0          0      48.69          0          0        640: 100%|██████████| 313/313 [01:05<00:00,  4.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:19<00:00,  8.05it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_object_segmentation \u001b[38;5;241m=\u001b[39m \u001b[43mobject_segmentation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/yolo_dataset_2.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:338\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:379\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    376\u001b[0m final_epoch \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopper\u001b[38;5;241m.\u001b[39mpossible_stop\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mval \u001b[38;5;129;01mor\u001b[39;00m final_epoch:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_metrics(metrics\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_loss_items(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr})\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopper(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:483\u001b[0m, in \u001b[0;36mBaseTrainer.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    Runs validation on test set using self.validator.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    The returned dict is expected to contain \"fitness\" key.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# use loss as fitness measure if not found\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_fitness \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_fitness \u001b[38;5;241m<\u001b[39m fitness:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/validator.py:186\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_predictions(batch, preds, batch_i)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_val_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_stats(stats)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed\u001b[38;5;241m.\u001b[39mkeys(), (x\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1E3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dt)))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/models/yolo/detect/val.py:145\u001b[0m, in \u001b[0;36mDetectionValidator.get_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stats) \u001b[38;5;129;01mand\u001b[39;00m stats[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;241m*\u001b[39mstats)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnt_per_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(\u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), minlength\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc)  \u001b[38;5;66;03m# number of targets per class\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mresults_dict\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"predictions = object_segmentation_model.predict(\"/kaggle/input/dataset-for-yolo/data/images/val/100.png\", save = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(\"/kaggle/working/runs/detect/train6/100.png\")\nplt.imshow(img)","metadata":{},"execution_count":null,"outputs":[]}]}